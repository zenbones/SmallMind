{"version":3,"sources":["webpack:///webpack/bootstrap","webpack:///./src/App.vue?0459","webpack:///./src/App.vue","webpack:///./src/router/index.js","webpack:///./src/store/index.js","webpack:///./src/main.js","webpack:///./src/views/Home.vue?82e6","webpack:///./src/components/Docs.vue?5bf4","webpack:///./src/assets/adoc/3.6.0/README.adoc","webpack:///src/components/Docs.vue","webpack:///./src/components/Docs.vue?0ac1","webpack:///./src/components/Docs.vue","webpack:///src/views/Home.vue","webpack:///./src/views/Home.vue?ef24","webpack:///./src/views/Home.vue"],"names":["webpackJsonpCallback","data","moduleId","chunkId","chunkIds","moreModules","executeModules","i","resolves","length","Object","prototype","hasOwnProperty","call","installedChunks","push","modules","parentJsonpFunction","shift","deferredModules","apply","checkDeferredModules","result","deferredModule","fulfilled","j","depId","splice","__webpack_require__","s","installedModules","exports","module","l","m","c","d","name","getter","o","defineProperty","enumerable","get","r","Symbol","toStringTag","value","t","mode","__esModule","ns","create","key","bind","n","object","property","p","jsonpArray","window","oldJsonpFunction","slice","render","_vm","this","_h","$createElement","_c","_self","attrs","_v","staticRenderFns","script","component","use","routes","path","Home","router","Store","state","mutations","actions","config","productionTip","store","h","App","$mount","domProps","_s","documentation"],"mappings":"aACE,SAASA,EAAqBC,GAQ7B,IAPA,IAMIC,EAAUC,EANVC,EAAWH,EAAK,GAChBI,EAAcJ,EAAK,GACnBK,EAAiBL,EAAK,GAIHM,EAAI,EAAGC,EAAW,GACpCD,EAAIH,EAASK,OAAQF,IACzBJ,EAAUC,EAASG,GAChBG,OAAOC,UAAUC,eAAeC,KAAKC,EAAiBX,IAAYW,EAAgBX,IACpFK,EAASO,KAAKD,EAAgBX,GAAS,IAExCW,EAAgBX,GAAW,EAE5B,IAAID,KAAYG,EACZK,OAAOC,UAAUC,eAAeC,KAAKR,EAAaH,KACpDc,EAAQd,GAAYG,EAAYH,IAG/Be,GAAqBA,EAAoBhB,GAE5C,MAAMO,EAASC,OACdD,EAASU,OAATV,GAOD,OAHAW,EAAgBJ,KAAKK,MAAMD,EAAiBb,GAAkB,IAGvDe,IAER,SAASA,IAER,IADA,IAAIC,EACIf,EAAI,EAAGA,EAAIY,EAAgBV,OAAQF,IAAK,CAG/C,IAFA,IAAIgB,EAAiBJ,EAAgBZ,GACjCiB,GAAY,EACRC,EAAI,EAAGA,EAAIF,EAAed,OAAQgB,IAAK,CAC9C,IAAIC,EAAQH,EAAeE,GACG,IAA3BX,EAAgBY,KAAcF,GAAY,GAE3CA,IACFL,EAAgBQ,OAAOpB,IAAK,GAC5Be,EAASM,EAAoBA,EAAoBC,EAAIN,EAAe,KAItE,OAAOD,EAIR,IAAIQ,EAAmB,GAKnBhB,EAAkB,CACrB,IAAO,GAGJK,EAAkB,GAGtB,SAASS,EAAoB1B,GAG5B,GAAG4B,EAAiB5B,GACnB,OAAO4B,EAAiB5B,GAAU6B,QAGnC,IAAIC,EAASF,EAAiB5B,GAAY,CACzCK,EAAGL,EACH+B,GAAG,EACHF,QAAS,IAUV,OANAf,EAAQd,GAAUW,KAAKmB,EAAOD,QAASC,EAAQA,EAAOD,QAASH,GAG/DI,EAAOC,GAAI,EAGJD,EAAOD,QAKfH,EAAoBM,EAAIlB,EAGxBY,EAAoBO,EAAIL,EAGxBF,EAAoBQ,EAAI,SAASL,EAASM,EAAMC,GAC3CV,EAAoBW,EAAER,EAASM,IAClC3B,OAAO8B,eAAeT,EAASM,EAAM,CAAEI,YAAY,EAAMC,IAAKJ,KAKhEV,EAAoBe,EAAI,SAASZ,GACX,qBAAXa,QAA0BA,OAAOC,aAC1CnC,OAAO8B,eAAeT,EAASa,OAAOC,YAAa,CAAEC,MAAO,WAE7DpC,OAAO8B,eAAeT,EAAS,aAAc,CAAEe,OAAO,KAQvDlB,EAAoBmB,EAAI,SAASD,EAAOE,GAEvC,GADU,EAAPA,IAAUF,EAAQlB,EAAoBkB,IAC/B,EAAPE,EAAU,OAAOF,EACpB,GAAW,EAAPE,GAA8B,kBAAVF,GAAsBA,GAASA,EAAMG,WAAY,OAAOH,EAChF,IAAII,EAAKxC,OAAOyC,OAAO,MAGvB,GAFAvB,EAAoBe,EAAEO,GACtBxC,OAAO8B,eAAeU,EAAI,UAAW,CAAET,YAAY,EAAMK,MAAOA,IACtD,EAAPE,GAA4B,iBAATF,EAAmB,IAAI,IAAIM,KAAON,EAAOlB,EAAoBQ,EAAEc,EAAIE,EAAK,SAASA,GAAO,OAAON,EAAMM,IAAQC,KAAK,KAAMD,IAC9I,OAAOF,GAIRtB,EAAoB0B,EAAI,SAAStB,GAChC,IAAIM,EAASN,GAAUA,EAAOiB,WAC7B,WAAwB,OAAOjB,EAAO,YACtC,WAA8B,OAAOA,GAEtC,OADAJ,EAAoBQ,EAAEE,EAAQ,IAAKA,GAC5BA,GAIRV,EAAoBW,EAAI,SAASgB,EAAQC,GAAY,OAAO9C,OAAOC,UAAUC,eAAeC,KAAK0C,EAAQC,IAGzG5B,EAAoB6B,EAAI,IAExB,IAAIC,EAAaC,OAAO,gBAAkBA,OAAO,iBAAmB,GAChEC,EAAmBF,EAAW3C,KAAKsC,KAAKK,GAC5CA,EAAW3C,KAAOf,EAClB0D,EAAaA,EAAWG,QACxB,IAAI,IAAItD,EAAI,EAAGA,EAAImD,EAAWjD,OAAQF,IAAKP,EAAqB0D,EAAWnD,IAC3E,IAAIU,EAAsB2C,EAI1BzC,EAAgBJ,KAAK,CAAC,EAAE,kBAEjBM,K,oGCvJLyC,EAAS,WAAa,IAAIC,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,MAAM,CAAC,GAAK,QAAQ,CAACF,EAAG,MAAM,CAACE,MAAM,CAAC,GAAK,QAAQ,CAACF,EAAG,cAAc,CAACE,MAAM,CAAC,GAAK,MAAM,CAACN,EAAIO,GAAG,WAAW,GAAGH,EAAG,gBAAgB,IACvOI,EAAkB,G,YCAlBC,EAAS,GAKTC,EAAY,eACdD,EACAV,EACAS,GACA,EACA,KACA,KACA,MAIa,EAAAE,E,gCCbf,OAAIC,IAAI,QAER,MAAMC,EAAS,CACb,CACEC,KAAM,IACNvC,KAAM,OACNoC,UAAWI,EAAA,YAEb,CACED,KAAM,SACNvC,KAAM,QAINoC,UAAW,WACT,OAAO,+CAKPK,EAAS,IAAI,OAAU,CAC3BH,WAGa,Q,YCzBf,OAAID,IAAI,QAEO,UAAI,OAAKK,MAAM,CAC5BC,MAAO,GACPC,UAAW,GACXC,QAAS,GACTlE,QAAS,KCJX,OAAImE,OAAOC,eAAgB,EAE3B,IAAI,OAAI,CACNN,OAAA,EACAO,QACAvB,OAAQ,SAAUwB,GAChB,OAAOA,EAAEC,MAEVC,OAAO,S,yCCbV,IAAI1B,EAAS,WAAa,IAAIC,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,SAC/FI,EAAkB,GCDlB,EAAS,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,OAAO,CAACsB,SAAS,CAAC,UAAY1B,EAAI2B,GAAG3B,EAAI4B,mBACxI,EAAkB,GCDP,oigECMf,oBAEA,OACE,OACE,MAAJ,CACM,cAAN,aACQ,QAAR,OACQ,YAAR,EACQ,KAAR,YCd6L,I,YCOzLlB,EAAY,eACd,EACA,EACA,GACA,EACA,KACA,KACA,MAIa,EAAAA,E,QCXf,GACE,KAAF,OACE,WAAF,CACI,SCVyL,ICOzL,EAAY,eACd,EACAX,EACAS,GACA,EACA,KACA,KACA,MAIa,e","file":"js/app.945d1fcf.js","sourcesContent":[" \t// install a JSONP callback for chunk loading\n \tfunction webpackJsonpCallback(data) {\n \t\tvar chunkIds = data[0];\n \t\tvar moreModules = data[1];\n \t\tvar executeModules = data[2];\n\n \t\t// add \"moreModules\" to the modules object,\n \t\t// then flag all \"chunkIds\" as loaded and fire callback\n \t\tvar moduleId, chunkId, i = 0, resolves = [];\n \t\tfor(;i < chunkIds.length; i++) {\n \t\t\tchunkId = chunkIds[i];\n \t\t\tif(Object.prototype.hasOwnProperty.call(installedChunks, chunkId) && installedChunks[chunkId]) {\n \t\t\t\tresolves.push(installedChunks[chunkId][0]);\n \t\t\t}\n \t\t\tinstalledChunks[chunkId] = 0;\n \t\t}\n \t\tfor(moduleId in moreModules) {\n \t\t\tif(Object.prototype.hasOwnProperty.call(moreModules, moduleId)) {\n \t\t\t\tmodules[moduleId] = moreModules[moduleId];\n \t\t\t}\n \t\t}\n \t\tif(parentJsonpFunction) parentJsonpFunction(data);\n\n \t\twhile(resolves.length) {\n \t\t\tresolves.shift()();\n \t\t}\n\n \t\t// add entry modules from loaded chunk to deferred list\n \t\tdeferredModules.push.apply(deferredModules, executeModules || []);\n\n \t\t// run deferred modules when all chunks ready\n \t\treturn checkDeferredModules();\n \t};\n \tfunction checkDeferredModules() {\n \t\tvar result;\n \t\tfor(var i = 0; i < deferredModules.length; i++) {\n \t\t\tvar deferredModule = deferredModules[i];\n \t\t\tvar fulfilled = true;\n \t\t\tfor(var j = 1; j < deferredModule.length; j++) {\n \t\t\t\tvar depId = deferredModule[j];\n \t\t\t\tif(installedChunks[depId] !== 0) fulfilled = false;\n \t\t\t}\n \t\t\tif(fulfilled) {\n \t\t\t\tdeferredModules.splice(i--, 1);\n \t\t\t\tresult = __webpack_require__(__webpack_require__.s = deferredModule[0]);\n \t\t\t}\n \t\t}\n\n \t\treturn result;\n \t}\n\n \t// The module cache\n \tvar installedModules = {};\n\n \t// object to store loaded and loading chunks\n \t// undefined = chunk not loaded, null = chunk preloaded/prefetched\n \t// Promise = chunk loading, 0 = chunk loaded\n \tvar installedChunks = {\n \t\t\"app\": 0\n \t};\n\n \tvar deferredModules = [];\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"/\";\n\n \tvar jsonpArray = window[\"webpackJsonp\"] = window[\"webpackJsonp\"] || [];\n \tvar oldJsonpFunction = jsonpArray.push.bind(jsonpArray);\n \tjsonpArray.push = webpackJsonpCallback;\n \tjsonpArray = jsonpArray.slice();\n \tfor(var i = 0; i < jsonpArray.length; i++) webpackJsonpCallback(jsonpArray[i]);\n \tvar parentJsonpFunction = oldJsonpFunction;\n\n\n \t// add entry module to deferred list\n \tdeferredModules.push([0,\"chunk-vendors\"]);\n \t// run deferred modules when ready\n \treturn checkDeferredModules();\n","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{attrs:{\"id\":\"app\"}},[_c('div',{attrs:{\"id\":\"nav\"}},[_c('router-link',{attrs:{\"to\":\"/\"}},[_vm._v(\"Home\")])],1),_c('router-view')],1)}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","import { render, staticRenderFns } from \"./App.vue?vue&type=template&id=5a29ce62&\"\nvar script = {}\n\n\n/* normalize component */\nimport normalizer from \"!../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","import Vue from \"vue\";\nimport VueRouter from \"vue-router\";\nimport Home from \"../views/Home.vue\";\n\nVue.use(VueRouter);\n\nconst routes = [\n  {\n    path: \"/\",\n    name: \"Home\",\n    component: Home\n  },\n  {\n    path: \"/about\",\n    name: \"About\",\n    // route level code-splitting\n    // this generates a separate chunk (about.[hash].js) for this route\n    // which is lazy-loaded when the route is visited.\n    component: function () {\n      return import(/* webpackChunkName: \"about\" */ \"../views/Home.vue\");\n    }\n  }\n];\n\nconst router = new VueRouter({\n  routes\n});\n\nexport default router;\n","import Vue from \"vue\";\nimport Vuex from \"vuex\";\n\nVue.use(Vuex);\n\nexport default new Vuex.Store({\n  state: {},\n  mutations: {},\n  actions: {},\n  modules: {}\n});\n","import Vue from \"vue\";\nimport App from \"./App.vue\";\nimport router from \"./router\";\nimport store from \"./store\";\n\nVue.config.productionTip = false;\n\nnew Vue({\n  router,\n  store,\n  render: function (h) {\n    return h(App);\n  }\n}).$mount(\"#app\");\n","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('Docs')}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('span',{domProps:{\"innerHTML\":_vm._s(_vm.documentation)}})}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","export default \"= Smallmind\\n:doctype: book\\n:toc: auto\\n:icons: font\\nA journey through glue code\\n\\n[preface]\\nThe SmallMind project is about all the pieces of functionality you need to take care of after you've written your business logic. It's about logging, pooling, monitoring, caching and remoting. It's about doing all this without assumptions or privilege, so that every piece can be dropped, adapted, or extended. This project is opinionated, but we hope those opinions come out of a thoughtful and positive place.\\n\\n== About\\n\\nThis is a work in progress, which is to say it's mostly code. We're open to opinions, modifications, questions, and, above all, help, because if there's any point to this project, it's to be helpful.\\n\\n=== Download\\n\\nAll Smallmind modules are pushed to Maven Central under the *org.smallmind* group id. All artifacts which are intended to work together have the same sem version. Explicit module listings can be found in the various sections of this document covering those modules. Wherever possible, dependencies of this project are marked as `<optional>true</optional>`. This does require projects using these libraries to include such dependencies along with the relevant Smallmind modules, but keeps this project from making unwanted decisions. If we've missed any such opportunities, please let us know.\\n\\n=== Configuration\\n\\nThere is none. IOC projects provide both the base container and configuration. Smallmind provides some occasionally helpful Spring beans, but there's no magic in them. Whatever flavor of dependency injection you prefer should work just as well.\\n\\n=== Modules\\n\\n* <<claxon>> - A unified monitoring framework with extensible meters and pluggable metric warehousing integrations (comes with Datadog, JMX, Logging and Prometheus).\\n* <<spark>> - Maven packaging formats for the construction of self-contained executable build artifacts.\\n\\n[[claxon, Claxon]]\\n= Claxon\\n\\n[partintro]\\nClaxon is a unified code instrumentation and metric gathering framework that abstracts away the underlying metric warehousing systems, allowing new systems to be plugged in and replaced as needed. It can be safely used as a dependency in libraries to provide instrumentation in a warehouse agnostic fashion, without forcing any particular notion of the underlying metrics storage system on the library adopter. New meters can be created without reference to the eventual warehousing system, and new warehouses can be integrated and used with all Claxon meters.\\n\\n== Install\\n\\nTo use Claxon you need the `claxon-registry` dependency, and you may add dependencies for any emitters you're interested in the various `claxon-emitter-*` modules. There's also a simple Http endpoint for scraping pull-based emitters in the `claxon-http` module.\\n\\n.Claxon Registry\\n[source,xml]\\n----\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>claxon-registry</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n----\\n\\n.Claxon Push-based Integrations\\n[source,xml]\\n----\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>claxon-emitter-datadog</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>claxon-emitter-jmx</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>claxon-emitter-logging</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n----\\n\\n[[claxon-maven-pull-based, Pull-based Integrations]]\\n.Claxon Pull-based Integrations\\n[source,xml]\\n----\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>claxon-emitter-prometheus</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>claxon-http</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n----\\n\\n== Acknowledgments\\n\\nThanks to Dropwizard Metrics, micrometer.io and HdrHistogram.\\n\\n== Design Choices\\n\\nMetric warehousing systems such as StatsD, InfluxDB or Prometheus all store and provide for the querying of time series data. Some systems allow for the calculation of aggregations (minimums, maximums, velocity, histograms, etc.) upon storing data points, or as part of the provided query capabilities, while others do not. Some systems we may want to integrate with, such as JMX, have no notion of a time series at all. A project such as this one might fulfill its goals by fixing its notion of the meters it allows, and then pushing the responsibility of implementing each of those meters onto the warehouse integrations. This would allow integrations with systems with a built-in notion of aggregations to take advantage of those capabilities. We have instead chosen to ignore most of the differences between warehousing systems, and separate the implementation of meters from the underlying capabilities of the various data stores. That doesn't mean that you can't choose to build your own meters and warehousing integrations that are more tightly coupled, it just means that the meters provided by this project all aggregate on the client, and are uniformly usable on both the provided warehouse integrations, or with new integrations you may choose to create. It means that this framework does not privilege its provided meters in any way, and all of its capabilities, including AOP instrumentation annotations, are extended to any meters you choose to create. Further, you can create new warehouse integrations without having to provide implementations for any meters at all, and those integrations will be usable with all meters which fulfill the contracts for this project.\\n\\n== Registry\\n\\nThe ClaxonRegistry holds both meters (`org.smallmind.claxon.registry.meter.Meter`) and warehouse integration bindings (`org.smallmind.claxon.registry.Emitter`). Meters should be obtained through the ClaxonRegistry, which accepts the MeterBuilder interface (org.smallmind.claxon.registry.meter.MeterBuilder) through its `register()` method, as opposed to meter implementations directly. MeterBuilder is intended to take only the necessary parameters for constructing instances of Meter, and to be itself lightweight, so that the cost of not building a meter (if already present in the registry) is negligible. If the construction of a meter can't be directly trivialized, this project provides a lazy indirection (`org.smallmind.claxon.registry.meter.LazyBuilder`) which can delay the use of a MeterBuilder until the point of necessity. On the back side, implementations of Emitter are bound by name through the registry's `bind()` method, and called with all aggregated metrics on the configured collection interval. Let's take a closer look at the useful bits...\\n\\n=== Bind\\n\\n[small]#`ClaxonRegistry bind(String name, Emitter emitter)`#\\n\\nInstalls and binds an emitter to the specified name, which will then start receiving updates. Reusing a name will rebind that name to the new emitter.\\n\\n[[claxon-registry-register, Register]]\\n=== Register\\n\\n[small]#`<M extends Meter> M register(Class<?> caller, MeterBuilder<M> builder, Tag... tags)`#\\n\\nCreates and registers a meter if it's not already registered, otherwise returns the currently registered meter. A meter's uniqueness is a combination of the caller's class and the tags passed as parameters to this method. Although the calling class is intended to be the direct caller of this method, that's not a requirement. Think of it the same way many logging systems use a class as the log name, which is intended, but not required to be the class doing the logging. In a similar fashion, whether this meter actually outputs any metrics, and the namespace of those metrics (as passed to this registries bound emitters), will depend upon the class of the caller (see <<claxon-configuration>>). Every possible Meter should have a MeterBuilder available. We recommend that a meter of type Foo have a builder in the same package named FooBuilder, and this is the practice followed in this project. This MeterBuilder makes up the second parameter to this method, followed by any tags, where a Tag is a simple key/value pair of strings.\\n\\n==== Example\\n\\nregistry.register(MyInstance.class, new GaugeBuilder(), new Tag(\\\"event\\\", \\\"update\\\"), new Tag(\\\"source\\\", \\\"mysql\\\"))\\n\\n[NOTE]\\nMany metric warehouses are capable of storing and querying multi-dimensional data, and it's the tags used in creating a metric which will become the dimensions of the time series formed by the metric's values. Where a warehouse system is not multi-dimensional, the tags will be used to determine the time series data's hierarchical namespace. In either case, it's best to use tags with consistent ordering and limited cardinality (see <<claxon-tags>> for further advice).\\n\\n=== Unregister\\n\\n[small]#`void unregister(Class<?> caller, Tag... tags)`#\\n\\nYou should probably avoid unregistering and re-registering a meter, but if you know that a meter will no longer be used, and should be available for garbage collection, you can call this method.\\n\\n=== Track _Observable_\\n\\n[small]#`<O extends Observable> O track(Class<?> caller, MeterBuilder<?> builder, O observable, Tag... tags)`#\\n\\nRegisters a meter that will track changes in an Observable. The Observable instance is only weakly referenced by the meter, so that if the meter is the only remaining reference to the Observable instance, the meter will be unregistered and both the Observable instance and the meter will be available for garbage collection.\\n\\n=== Track _Lambda_\\n\\n[small]#`<T> T track(Class<?> caller, MeterBuilder<?> builder, T measured, Function<T, Long> measurement, Tag... tags)`#\\n\\nRegisters a meter that will poll the state of a 'measured' instance on the registry's collection interval using the supplied measurement function. Much like the tracking of Observables above, the measured instance is weakly held, such that if the meter is the only remaining reference, the meter will be unregistered and both the measured instance and the meter will be available for garbage collection.\\n\\n[[claxon-configuration, Configuration]]\\n=== Configuration\\n\\nThe configuration for Claxon is simple and, like all SmallMind modules, programmatic. A configuration instance is composed of the following types...\\n\\n* *Clock* (_clock_) - A clock can provide both wall time (think milliseconds since the epoch) and monotonic time (think nanoseconds). The default clock should work perfectly well, so you should not normally need to set your own.\\n* *Stint* (_collectionStint_) - The collection interval. The default value represents a 2-second interval.\\n* *Tag[]* (_registryTags_) - A set of default tags which are to be added to every meter. The default value is empty.\\n* *NamingStrategy* (_namingStrategy_) - The logic by which the `Class<?> caller` of meter registrations (see <<claxon-registry-register>> above) are turned into the root names of the quantities emitted by those meters (see <<claxon-meters>> below). The default value is the <<claxon-configuration-implied-naming-strategy>>.\\n\\n[[claxon-naming-strategy, NamingStrategy]]\\n==== NamingStrategy\\n\\nThe purpose of meters is to emit metrics, or _quantities_ in the parlance of this project (see <<claxon-meters-quantities>> below). Every registered meter has a root name, and every quantity emitted by a meter has a name. The concatenation of the meter's root name with each quantity's name will form the _full_ name of that quantity passed to each of the registry's bound <<claxon-emitters>>. For most types of metric warehouses, the full name of each quantity will form the namespace of the time series created by that quantity's values. Because the emitted quantity namespaces will generally have limited cardinality (for the root names of meters and certainly for the quantity names), it's the Tags submitted with the registration which must guarantee the uniqueness of those namespaces. The root name of a meter is determined by the `Class<?> caller` parameter passed to the `register()` method (see <<claxon-registry-register>> above) via the NamingStrategy (`org.smallmind.claxon.registry.NamingStrategy`) in the registry's configuration. There are two naming strategies included in this project, the *ObviousNamingStrategy* and the *ImpliedNamingStrategy*.\\n\\n===== ObviousNamingStrategy\\n\\n[small]#`org.smallmind.claxon.registry.ObviousNamingStrategy``#\\n\\nThe ObviousNamingStrategy holds a Set of <<claxon-dot-notation>> instances which it attempts to match against the names of the caller classes submitted with meter registrations. If any match can be found, then the *full name of the caller class* becomes the root name of the registered meter. If no match can be found, then no meter will be registered, and no metrics will be emitted.\\n\\n[TIP]\\nThe diversity of this naming should ease the pressure on submitted Tag sets to guarantee the unique namespaces of warehoused time series, however, creators of libraries using this project should not count on this, as it's the choice of the library consumer. It's best to generate Tag sets with enough total cardinality to guarantee unique time series namespaces in their own right.\\n\\n[[claxon-configuration-implied-naming-strategy, ImpliedNamingStrategy]]\\n===== ImpliedNamingStrategy\\n\\n[small]#`org.smallmind.claxon.registry.ImpliedNamingStrategy`#\\n\\nThe ImpliedNamingStrategy holds a Map of _<<claxon-dot-notation>> to String_ entries. This strategy attempts to match the dot notated keys against the names of the caller classes submitted with meter registrations. If no match can be found, then no meter will be registered, and no metrics will be emitted. If one or more matches are found, then the strongest match, which is the one matching the most dot notated segments in the name, will determine the root name for that meter, which will be the String *value* of the winning map entry. Although there's no constraint on the values of this strategy's mappings, it's recommended to use dot notated names. This is the default naming strategy for Claxon configurations.\\n\\n[TIP]\\nIf you wanted to emit all metrics with a single root name you could add a single _prefixMap_ entry like `put(new DotNotation(\\\"*\\\"), \\\"my.metrics\\\")`. Multiple entries can map to the same value, but even if each entry maps to a unique root name, the resulting namespaces are likely to be crowded. It is, therefore, very important that the Tag set registered for each meter guarantee the appropriate differentiation.\\n\\n[[claxon-dot-notation, DotNotation]]\\n===== DotNotation\\n\\n[small]#`org.smallmind.nutsnbolts.util.DotNotation`#\\n\\nA DotNotation instance represents a pattern match of '.' separated segments, with 2 possible wildcards.\\n\\n* *?* - Represents any single segment.\\n* *** - Represents any number of segments.\\n\\nThe greater the number of segments in the matching pattern, the stronger the match is considered, with an exact match counting slightly more than a wildcard match, which will roughly translate to the longest match, by segments, with the fewest wild cards.\\n\\n===== Examples\\n\\nThe pattern \\\"com.my.names' would match only the exact string, \\\"com.my.names\\\", while the pattern \\\"com.my.?.names.*\\\" would match any dot notated string starting with \\\"com.my.\\\" followed by any single segment (a series of characters which was not a '.'), followed by at least one (or any greater number) of dot notated segments.\\n\\n=== Instrumentation\\n\\nTo make use of this project, you could pass around a registry instance and directly call the `register()` method on it as needed, but that would be less than convenient. Instead, it's easier to interact with the registry through the static methods of the Instrument (`org.smallmind.claxon.registry.Instrument`) class. Instrument uses a thread local context to gain access to the underlying registry, while presenting both a direct access interface that mimics the `register()` and `track()` methods, as well as constructs for wrapping blocks of code with timing-based metrics. Turning on this functionality involves two bits of setup.\\n\\n. Create an instance of PerApplicationContext (`org.smallmind.nutsnbolts.lang.PerApplicationContext`) which will not be subject to garbage collection for the life of the registry. The easiest way to do this is to create it as a _bean_ within the same IOC (inversion of control) context that you use to configure the Claxon registry instance.\\n+\\n[TIP]\\n====\\nAs old fashioned as this may seem, in Spring xml this would be as simple as...\\n\\n[source,xml]\\n----\\n<bean id=\\\"perApplicationContext\\\" class=\\\"org.smallmind.nutsnbolts.lang.PerApplicationContext\\\"/>\\n----\\n====\\n\\n. Call the `initializeInstrumentation()` method of the configured ClaxonRegistry instance.\\n+\\n[TIP]\\nThe provided `org.smallmind.claxon.registry.spring.ClaxonRegistryFactoryBean` does this for you, if you're using Spring.\\n\\n==== With\\n\\n[small]#`Instrumentation with (Class<?> caller, MeterBuilder<?> builder, Tag... tags)`#\\n\\nThe prerequisites taken care of, Instrument functionality is accessed through its `with()` method. This method takes the same _caller_, _builder_ and _tags_ parameters as the `register()` method discussed previously (see <<claxon-registry-register>> above), but returns an instance of the Instrumentation (`org.smallmind.claxon.registry.Instrumentation`) interface. Through this interface you can...\\n\\n* `track()` to follow either an Obervable object, or any object coupled with a measuring function, as you can with the ClaxonRegistry directly.\\n+\\n[NOTE]\\n====\\n[source,java]\\n----\\nLinkedList<?> myList = new LinkedList();\\nInstrument.with(MyClass.class, new GaugeBuilder(), new Tag(\\\"pool\\\", \\\"used\\\")).track(myList, list -> (long)list.size());\\n----\\n====\\n\\n* `update()` the registered meter with a value, either a simple long value in the default time units (milliseconds), or with an explicit TimeUnit.\\n+\\n[NOTE]\\n====\\n[source,java]\\n----\\nInstrument.with(MyClass.class, new GaugeBuilder(), new Tag(\\\"event\\\", \\\"myevent\\\")).update(12345);\\n----\\n====\\n\\n* call `as()` to set the default time unit of the Instrumentation instance.\\n* call `on()` to wrap a block of code in timing metrics (passed as a Lambda with or without a return value).\\n+\\n[NOTE]\\n====\\n[source,java]\\n----\\nInstrument.with(MyClass.class, new SpeedometerBuilder(), new Tag(\\\"event\\\", \\\"myevent\\\"),new Tag(\\\"service\\\", \\\"myservice\\\")).on(() -> {\\n  ...\\n  instrumented code\\n  ...\\n});\\n----\\n====\\n\\n==== Annotations\\n\\nAlthough Instrument/Instrumentation together present a fairly simple and fluent interface, Claxon also allows wrapping methods in timing-based metrics via the use of annotations. You can do this not only with the meters which come built in, but also any you might develop.\\n\\n===== @Instrumented\\n\\n[small]#`org.smallmind.claxon.registry.aop.Instrumented`#\\n\\nThe root annotation is @Instrumented, which can be applied to both methods and constructors, and takes the following values...\\n\\n* `Class<?> caller () default Instrumented.class` - The caller which will be passed to the meter registration.\\n* `ConstantTag[] constants () default {}` - An array of <<claxon-instrumentation-constant-tag>> which defines those tags with constant values that will be passed to the meter registration.\\n* `ParameterTag[] parameters () default {}` - An array of <<claxon-instrumentation-parameter-tag>> which defines those tags whose values will be pulled from the parameters of the annotated method, and then passed to the meter registration.\\n* `TimeUnit timeUnit () default TimeUnit.MILLISECONDS` - The time units for the `update()` to the registered meter.\\n* `boolean active () default true` - Whether this meter is active. If this value is false, no timing update will occur.\\n* `Class<? extends InstrumentedParser<?>> parser ()` - The class of the <<claxon-instrumentation-instrumented-parser>> which will be used to decode the json string from this annotation (see the json value next), in order to produce a MeterBuilder that will be passed to the meter registration.\\n* `String json () default \\\"{}\\\"` - The json formatted string representing the meter to be registered via this annotation.\\n\\n[[claxon-instrumentation-constant-tag, @ConstantTag]]\\n====== @ConstantTag\\n\\nRepresents a tag whose keys and values are simple string constants.\\n\\n[[claxon-instrumentation-parameter-tag, @ParameterTag]]\\n====== @ParameterTag\\n\\nRepresents a tag whose keys are string constants, but whose values are the names of parameters of the annotated method, and whose values will be pulled from those parameters (via their `toString()` methods).\\n\\n[[claxon-instrumentation-instrumented-parser, InstrumentedParser]]\\n====== InstrumentedParser\\n\\n[small]#`org.smallmind.claxon.registry.aop.InstrumentedParser`#\\n\\nA json parser that accepts a json formatted string and returns an instance of MeterBuilder. Because the parsing of json is a more heavy-weight process that may be repeated many, many times, implementations of this interface will not actually be called unless a new Meter instance is to be constructed, which will only happen if the registry does not already contain an instance matching the caller class and tags. To allow meters you design to be used in @Instrumented annotations, all you need do is create an implementation of this interface and publish its json format and requirements.\\n\\n====== Example\\n\\nThe following would register a Histogram (`org.smallmind.claxon.registry.meter.Histogram`) and update that meter with the time `myMethod()` takes to execute (in the default time unit of milliseconds). The tags would have the set \\\"const1\\\", \\\"param1\\\" and \\\"param2\\\", where `param1` and `param2` would take their values from the method parameters (_parameter1_ and _parameter2_). The histogram would be built with 2 significant digits of storage, lowest discernible value of 1, highest of 3600000 and tracking percentiles at 75%, 95% and 99%...\\n\\n[source,java]\\n----\\n@Instrumented(\\n  caller = MyClass.class,\\n  constants = @ConstantTag(key = \\\"const1\\\", constant = \\\"value\\\"),\\n  parameters = {@ParameterTag(key = \\\"param1\\\", parameter = \\\"parameter1\\\"), @ParameterTag(key = \\\"param2\\\", parameter = \\\"parameter2\\\")},\\n  parser = HistogramParser.class,\\n  json = \\\"{\\\\\\\"numberOfSignificantValueDigits\\\\\\\": 2, \\\\\\\"lowestDiscernibleValue\\\\\\\": 1, \\\\\\\"highestTrackableValue\\\\\\\": 3600000, \\\\\\\"percentiles\\\\\\\": [{\\\\\\\"name\\\\\\\": \\\\\\\"p75\\\\\\\", \\\\\\\"value\\\\\\\": 75.0}, {\\\\\\\"name\\\\\\\": \\\\\\\"p95\\\\\\\", \\\\\\\"value\\\\\\\": 95.0}, {\\\\\\\"name\\\\\\\": \\\\\\\"p99\\\\\\\", \\\\\\\"value\\\\\\\": 99.0}]}\\\"\\n)\\npublic SomeClass myMethod (String parameter1, int parameter2) {\\n  ...\\n  instrumented code\\n  ...\\n}\\n----\\n\\n[[claxon-meters, Meters]]\\n== Meters\\n\\n[small]#`org.smallmind.claxon.registry.meter.Meter`#\\n\\nA meter in Claxon is an interface with two methods to implement, `update()` and `record()`.\\n\\n* `void update (long value)` - This method takes a long value and updates the meter. What that means is entirely dependent on the meter, but, generally, the meter will be tracking some series of aggregations over time, to which the updated value will be appended.\\n* `Quantity[] record ()` - When this method is called by the registry, on the collection interval, the meter should return an array of Quantity instances holding the names and values of its aggregations (or whatever other quantities the meter is designed to track).\\n\\nPretty simple. The only complexity is that meters should be multi-thread safe and, as far as possible, lock free and wait free. Multiple threads may be calling the `update()` method at any one time, and, while only one thread *should* be calling `record()`, we don't want updates waiting on each other any more than necessary. Nor should record calls block update operations, and update calls should definitely *not* block record operations. On top of this, a meter must be ready to aggregate its updates in between collection cycles so as not to lose data. And because there's no guarantee of the exact timing of collection cycles, reporting of aggregates that are stated per time unit should internally track the time passed since the last collection, in order to be as accurate as possible.\\n\\n[TIP]\\nThe Meter implementations in this project use a series of helper classes in the `org.smallmind.claxon.registry.aggregate` package. You may find these helpers useful when designing your own meters as well. You may also want to look at the meter implementations themselves in `org.smallmind.claxon.registry.meter`. Not that the code is any good, but we can always use the help making it better.\\n\\n[[claxon-meters-quantities, Quantities]]\\n=== Quantities\\n\\n[small]#`org.smallmind.claxon.registry.Quantity`#\\n\\nA quantity is a just container for a String name and a double value. The name of each quantity will be concatenated with the root name provided by the emitting meter to create a namespace for the value that's passed to each warehouse emitter. Exactly how this name is expressed, along with the meter's tags, is up to the logic within each emitter (see <<claxon-emitters>> below). We recommend that quantity names by kept simple. You can distinguish multi-word names via dot notation, hyphens, camel case, or simply smooshing them all together, but be prepared for the emitters to less sophisticated systems to mess with your naming, so simpler is better. We recommend sticking to dot notated names in most cases.\\n\\n=== Out Of The Box\\n\\nThis project includes a set of Meters in the `org.smallmind.claxon.registry.meter` package.\\n\\n* *Gauge* - Emits the \\\"minimum\\\", \\\"maximum\\\" and \\\"average\\\" of the updated values over the collection cycle.\\n* *Histogram* - Emits the \\\"count\\\" of updates, their \\\"velocity\\\", the \\\"minimum\\\", \\\"maximum\\\", \\\"mean\\\", and a set of specified quantiles over the collection cycle.\\n* *Speedometer* - Emits the \\\"minimum\\\", \\\"maximum\\\" and \\\"velocity\\\" of the updated values over the collection cycle.\\n* *Tachometer* - Emits just the \\\"velocity\\\" of the updated values over the collection cycle.\\n* *Tally* - Emits the running total of all updated values (which can be both positive and negative) over the life of the meter.\\n* *Trace* - Emits the exponential decaying average of the updated values over a set of specified time windows.\\n\\n[[claxon-tags, Tags]]\\n== Tags\\n\\n[small]#`org.smallmind.claxon.registry.Tag`#\\n\\nA tag is a String name and value. Emitters for warehouses capable of multi-dimensional indexing can usually pass tags directly through to the underlying system. Emitters that represent warehouses that hold time series data, but are not multi-dimensional, will have to use the tags as part of the hierarchical namespace created for each emitted quantity. Other systems will do what they can to create the best experience possible. You can help this process by trying to stick to a few rules...\\n\\n* Tag names should be kept simple, and, where multi-word names must be used, dot notating is probably best.\\n* Tag values should have low cardinality where possible. It makes the eventual use of the underlying systems easier. Obviously, a cardinality of 1 is probably too low, and should be either omitted or included in the quantity namespace.\\n* The Tag set should represent a robust uniqueness, by which we mean that it's unlikely to be duplicated, as a totality, by some other library that happens to be included in the same project. In the end, the client should be able to sort out the proper namespaces given package naming conventions and the available <<claxon-naming-strategy>> implementations. However, each library doing its part will make the whole that much easier to work with.\\n\\n[[claxon-emitters, Emitters]]\\n== Emitters\\n\\n[small]#`org.smallmind.claxon.registry.Emitter`#\\n\\nThere are essentially 2 different ways that metric warehouses ingest data, via either push (such as over a socket, REST API, or by method call) or pull (such as an HTTP scrape endpoint). Although you could implement the Emitter interface directly, we advise extending either the <<claxon-emitters-push-emiiter>> or <<claxon-emitters-pull-emiiter>> abstract classes. In either case, the only requirement is implementing the `record()` method.\\n\\n* `void record (String meterName, Tag[] tags, Quantity[] quantities)` - The record method is called on each collection cycle once for each registered meter. It's passed the root name of the meter (as determined by the <<claxon-naming-strategy>> in force), an array of the tags registered for that meter, and an array of the quantities being emitted. Whatever an emitter chooses to do with this information should be done efficiently and without blocking. It's up to an emitter to handle any long running operations in an asynchronous fashion, sensitive to the fact that record might be called again before the current asynchronous operation is complete.\\n\\n[[claxon-emitters-push-emiiter, PushEmitter]]\\n=== PushEmitter\\n\\nA PushEmitter can generally take the information in the record call and translate it to the underlying warehouse system, assuming such calls are efficient in their own right.\\n\\n[small]#`org.smallmind.claxon.registry.PushEmitter`#\\n\\n[[claxon-emitters-pull-emiiter, PullEmitter]]\\n=== PullEmitter\\n\\n[small]#`org.smallmind.claxon.registry.PullEmitter`#\\n\\nA PullEmitter will have to store and forward incoming time series data by necessity. Given that Claxon expects meters to aggregate updates between collection intervals, pull-based emitters may be receiving multiple sets of aggregates before being able to pass that data on to the underlying warehouse, depending on the timing of Claxon collection cycles and those of the underlying system. As the methods of aggregation, if any, will vary from meter to meter, a pull-based emitter can't simply squash aggregates together, and should be prepared to send them on as independent values, along with their original timestamps.\\n\\n==== EmitterResource\\n\\n[small]#`org.smallmind.claxon.http.EmitterResource`#\\n\\nClaxon provides a generic Jersey REST API endpoint for pull-based emitters, such as Prometheus, available in the *claxon-http* artifact (see <<claxon-maven-pull-based>> above). If the provided implementation is not appropriate for your project, building your own should be simple enough considering the few lines of code it took to implement ours.\\n\\n=== Out Of The Box\\n\\nClaxon provides emitters for the following systems as part of this project...\\n\\n==== DataDog\\n\\nThis is fairly straightforward. The integration uses DataDog's StatsD capabilities, which, being UDP-based, is very fast, and cares little about the choice of characters in its namespace or tags. It's push capable, allowing aggregations to be forwarded as necessary. DataDog prefers dot notated names, in keeping with the general precedent set by almost every programming language, which lowers the impedance mismatch which might otherwise occur. All quantities are output as gauge values, as all aggregation is assumed to be handled client side. All of this should lead to an experience with few surprises.\\n\\n==== JMX\\n\\nJMX knows nothing about time series, so the resulting management beans will contain only the last known data points. The translation is otherwise pretty direct, with dot notated meter names making up the JMX domain name, tags creating the attendant properties, and quantity names translating to available fields.\\n\\n==== Message\\n\\nA very simple translation to string values handed off to a `Consumer<String>`, intended for easy integration with logging systems.\\n\\n==== Prometheus\\n\\nBeware when using Prometheus that its scrape protocol is limited. You might think it would be easy enough to allow multiple data points for any single time series, within the same scrape Http body, by referencing the timestamp value the protocol allows to be included with each data point, and, where that was either not present or not sufficient, relying on the simple ordering of the values themselves. However, that is not the case and multiple data points for the same series will be lost. Due to this limitation, Claxon's Prometheus integration will only report the last data point received for any series since the last scrape request. It's important, therefore, to have your Prometheus instance scrape the Claxon endpoint at least as often as Claxon is set to report its aggregated values. In practical terms...\\n\\n[WARNING]\\nThe Prometheus collection interval must be less than or equal to the Claxon collection interval as defined by `ClaxonConfiguration.getCollectionStint()`, _or you may lose data points_.\\n\\n===== Naming\\n\\nPrometheus does not allow dot notated names, nor anything but the most basic ascii characters and numbers. The more natural dot notated names used by Claxon will be translated to underscore separated names. Most everything otherwise inadmissible for prometheus will also end up as underscores. Fortunately, prometheus is muti-dimensional and Claxon's guidelines for tags are in line with those promulgated by Prometheus.\\n\\n[[scribe, Scribe]]\\n= Scribe\\n\\n[partintro]\\nScribe is a logging framework with no specific dependency on a logger endpoint. It can be safely used in libraries without forcing any particular notion of the endpoint logging system on the library adopter. It is programmatically configurable and has no proprietary configuration format. Scribe can consume and integrate both Apache Commons Logging and SLF4J logging. Scribe's adapters for endpoint logging systems are written as services, so, when choosing a logging endpoint in top-level projects, it's enough to include the desired module as a project dependency. Scribe has a notion of thread local context which flows through to the context implementations of endpoint logger adapters. Scribe is fast, efficient, and easy to extend. The implementation requirements for adapters, appenders, filters and formatters are clear and minimal.\\n\\n== Install\\n\\nTo use Scribe you'll need the `scribe-pen` dependency, and, if using an endpoint logger, you'll need to add the appropriate `scribe-ink-*` module (these modules describe services, so use only the single dependency that matches your endpoint logging framework). Integration with Apache Commons Logging is achieved through adding the `scribe-apache` module, and with SLF4J by adding the `scribe-slf4j` module.\\n\\n.Scribe Pen\\n[source,xml]\\n----\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>scribe-pen</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n----\\n\\n.Scribe Ink\\n[source,xml]\\n----\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>scribe-ink-indigenous</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>scribe-ink-jdk</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>scribe-ink-log4j</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n----\\n\\n.Scribe Integrations\\n[source,xml]\\n----\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>scribe-apache</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n<dependency>\\n  <groupId>org.smallmind</groupId>\\n  <artifactId>scribe-slf4j</artifactId>\\n  <version>LATEST</version>\\n</dependency>\\n----\\n\\n== Acknowledgments\\n\\nThanks to SLF4J.\\n\\n== Design Choices\\n\\nThere are a few simple reasons for designing yet another logging framework almost no one will ever use...\\n\\n* Programmatic configuration at its heart. Nothing but dependency injection.\\n* Simple implementation of extensions with no priveleged internals and minimal biases.\\n* No logging level checks necessary because expensive operations are delayed until the output stage.\\n* Where an expensive operation is necessary, it can be defined in a way which allows its execution to be delayed until the output stage.\\n* Native message formatting using the _printf_ style from `String.format()`.\\n* The last argument in *all* logging methods is the _var args_ for message formatting, and this includes those methods which take an Exception (so you can add an exception without losing formatted messages).\\n\\nAnd that last reason is honestly the one which started this project. Hopefully it's not the only one for using it, but, for us, it's enough.\\n\\n[[scribe-logger, Logger]]\\n== Logger\\n\\n[small]#`org.smallmind.scribe.pen.Logger`#\\n\\nThe mechanism of logging in Scribe is a Logger. A logger has a <<scribe-logger-name>> and a <<scribe-level>> at which it will log, may have a <<scribe-logger-context>>, holds <<scribe-appenders>>, <<scribe-filters>> and <<scribe-enhancers>>, and can decorate logs with <<scribe-parameters>>. The basic function of a Logger is expressed through its `log()` methods....\\n\\n* `void log (Level level, String message, Object... args)`\\n* `void log (Level level, Throwable throwable)`\\n* `void log (Level level, Throwable throwable, String message, Object... args)`\\n* `void log (Level level, Object object)`\\n* `void log (Level level, Throwable throwable, Object object)`\\n* `void log (Level level, Supplier<String> supplier)`\\n* `void log (Level level, Throwable throwable, Supplier<String> supplier)`\\n\\nEach log statement has a _<<scribe-level>>_ at which it will be logged, can take a _Throwable_ to provide the stack trace output of an error, and takes one of three forms of message body...\\n\\n* *Formatted* - Formatted messages take a base _String_ message and an optional set of arguments. If any arguments are passed, the message will be interpreted as a formatting template via the `String.format()` method, with the arguments passed as the formatting arguments.\\n* *Object* - If a single _Object_ is provided as the log message, the output will be the result of the object's `toString()` method. As message interpolation is delayed until after all filtering, this is one way of guaranteeing that complex or expensive operations to generate a log message will not be performed unless they are needed.\\n* *Supplier* - If a single _Supplier<String>_ is provided as the log message, the ouput will be the result of the supplier's `get()` method. As with using a simple _Object_ above, this can be used to delay complex or expensive operations until they are required.\\n\\nFor the sake of clarity and convenience, the above methods are replicated, once for each available log <<scribe-level>> (sans the _Level_ parameter, obviously), as in...\\n\\n.An Info level log with a formatted messge\\n[source,java]\\n----\\nvoid info (String message, Object... args)\\n----\\n\\n...or...\\n\\n.A Warn level log with a _Throwable_ and _Supplier_\\n[source,java]\\n----\\nvoid warn (Throwable throwable, Supplier<String> supplier)\\n----\\n\\n[[scribe-logger-name, Name]]\\n=== Name\\n\\nEvery logger instance has a name by which it's retrieved (see <<scribe-logger-manager>> below), by which its configuration may be fine-tuned (see <<scribe-templates>> below), and by which it may be known in log output. Although the choice of name is yours, it's suggested that you stick with the name of the class doing the logging.\\n\\n[[scribe-level, Level]]\\n=== Level\\n\\n[small]#`org.smallmind.scribe.pen.Level`#\\n\\nThe available log levels, which provide for basic categorization and filtering of log entries, are represented by the *Level* enum, which contains the following ordinal values...\\n\\n* *TRACE* - Intended for very fine gained more-than-debug logging.\\n* *DEBUG* - For traditional debug logs.\\n* *INFO* - Informational messages, for example service startups and shutdowns, initial conditions, etc.\\n* *WARN* - Intended for possible problems or misconfigurations which do not rise to the level of overt errors.\\n* *ERROR* - A notice that something has gone wrong, often including an exception.\\n* *FATAL* - Intended for world-stopping events which cause shutdown or other unrecoverable states.\\n* *OFF* - Do not log. For use on <<scribe-logger>> instances or <<scribe-appenders>>, in order to shut them off. This should *never* be used as the level of a `log()` event.\\n\\n[[scribe-logger-context, LoggerContext]]\\n=== LoggerContext\\n\\n[small]#`LoggerContext`#\\n\\nSome loggers are capable of adding information about the point in the code at which logging was initiated. This 'logger context' may include...\\n\\n* `String getClassName()` - The class which in which the logger was called.\\n* `String getMethodName()` - The method from which the logger was called.\\n* `String getFileName()` - The name of the file containing the code which initiated the logging call.\\n* `int getLineNumber()` - The line number of the initiating call in the file containing the calling code.\\n* `boolean isNativeMethod()` - Whether the valling code represents a native method (via C interface).\\n\\nThere is some overhead to gathering this information, so the capability to automatically include it can but turned on/off via the `setAutoFillLoggerContext()` method on the logger.\\n\\n[[scribe-appenders, Appenders]]\\n=== Appenders\\n\\n[small]#`org.smallmind.scribe.pen.Appender`#\\n\\nAppenders are the vehicle for publishing log records. What 'publishing' means is up to the appender implementation. Appenders can take <<scribe-filters>>, and implementations should honor them. An appender can be set inactive, via the `setActive()` method, and inactive appenders should not output log records. An appender can also have an instance of <<scribe-error-handler>> set on it, which will be called upon to handle uncaught errors in the appender's `publish()` method.\\n\\n[[scribe-formatted-appender, FormattedAppender]]\\n==== Formatted Appender\\n\\n[small]#`org.smallmind.scribe.pen.FormattedAppender`#\\n\\nA FormattedAppender takes an implementation of <<scribe-formatter>> which will determine the structure of its output (as defined by the appender).\\n\\n[[scribe-formatter, Formatter]]\\n===== Formatter\\n\\n[small]#`org.smallmind.scribe.pen.Formatter`#\\n\\nA formatter takes a log <<scribe-record>> and returns a string to be output by an appender by fulfilling...\\n\\n[source,java]\\n----\\nString format (Record record)\\n  throws Exception;\\n----\\n\\nThere are a couple of useful formatters included with this project.\\n\\n[[scribe-xml-formatter, XMLFormatter]]\\n====== XMLFormatter\\n\\n[small]#`org.smallmind.scribe.pen.XMLFormatter`#\\n\\nThe XMLFormatter provides, unsurprisingly, an xml formatted output. There are a few attributes you can set on the formatter to configure its behavior...\\n\\n* *timestamp* (_org.smallmind.scribe.pen.Timestamp_) - Controls how dates are formatted. Defaults to \\\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\\\".\\n* *newLine* (_String_) - The line separator used when pretty printing the output. Defaults to `System.getProperty(\\\"line.separator\\\")`.\\n* *cdata* (_boolean_) - If true then the output of any stack trace will be wrapped in `<![CDATA[ ... ]]>` markers. Defaults to false.\\n* *indent* (_int_) - The number of spaces used when pretty printing the output. Defaults to a 3 space indent.\\n* *recordElements* (_org.smallmind.scribe.pen.RecordElement[]_) - An array of enum values representing the elements which will be included in the output. Defaults to all of the below...\\n** *DATE* - The date this log record was emitted.\\n** *LEVEL* - The <<scribe-level>> at which this log record was emitted.\\n** *LOGGER_NAME* - The name of the logger emitting this record.\\n** *LOGGER_CONTEXT* - The <<scribe-logger-context>> of this log record (if available).\\n** *MESSAGE* - The log message attached to this record.\\n** *MILLISECONDS* - The epoch milliseconds at which this record was emitted.\\n** *PARAMETERS* - The <<scribe-parameters>> available to this record.\\n** *STACK_TRACE* - The stack trace of any _Throwable_ set on this record.\\n** *THREAD* - Information about the thread which carried this logging call.\\n\\n====== PatternFormatter\\n\\n[small]#`org.smallmind.scribe.pen.PatternFormatter`#\\n\\nThe pattern formatter is a flexible log record formatter with a traditional output style. This formatter takes only two configuration parameters...\\n\\n* *timestamp* (_org.smallmind.scribe.pen.Timestamp_) - Controls how dates are formatted. Defaults to \\\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\\\".\\n* *format* (_String_) - The operation of the format string is similar to that of _String_ formatting flags...\\n+\\n====\\nPortrayed as a regular expression, each flag has the general form of...\\n\\n[source,regexp]\\n----\\n({[^%]+)?%((+|-)?(\\\\d+))?(.\\\\d*)?(!(+|-)[^!]*!)?([dtnlmTCMNLFsp])([^}]+})?\\n----\\n\\nLet's take this apart piece by piece...\\n\\n. *{_header_* - An optional header starts with `{` followed by any text which does not contain a `%`.\\n. *%* - The `%` declares a formatting field which will be substituted according to the possible conversions (see below).\\n. *``+|-``_width_* - Sets the maximum field length, where the optional ``+`` or ``-`` is used to denote a right or left padded field, if the field length is less than the width specifier. If this segment is absent, then no padding will be used.\\n. *._precision_* - An optional precision starts with a `.` and is used in the dot notated fields (logger name `n` and context class `C`) to specify a maximum number of segments to display, starting from the right. The precision specifier is also used in the multi-line conversion fields (currently just parameters `p`), to specify the maximum number of lines displayed (as a multi-line list). The precision specifier will be ignored on all other field types.\\n+\\n[NOTE]\\nFor example, given a logger name of `com.mydomain.myproject.MyClass` and a format flag of `%.2n`, the conversion would print `myproject.MyClass`.\\n. *!``+|-``_prefix_!* - The `!...!` markers specify a line separator for, and optional prefix text to insert before, each line of a multi-line field (parameters `p`). The `+` or `-` is required, and sets whether the *first* line should also be prefixed with the text (`+` for true and `-` for false).\\n+\\n[NOTE]\\nFor example, the marker `!-,\\\\n!` would tell the formatter to insert a comma followed by a line-break before each line of a multi-line field, _excluding_ the first, which would present a comma separated list. The default used is equivalent to `!+\\\\n\\\\t!`, or a new-line followed by a tab starting each output line, _including_ the first.\\n. *conversion* - The available conversion flags are...\\n* *d* - The date stamp of the log entry (defaults to yyyy-MM-dd'T'HH:mm:ss.SSSZ).\\n* *t* - The time stamp of the entry in milliseconds.\\n* *n* - The logger name.\\n* *l* - The logger <<scribe-level>>.\\n* *m* - The log message.\\n* *T* - The name of the thread in which the logging occurred (if available).\\n* *C* - The class from which the log event was issued (if available).\\n* *M* - The method in which the log event was issued (if available).\\n* *N* - Whether the method which issued the log event was native code or not [true or false] (if available).\\n* *L* - The line number in the class file from which the log event was issued (if available).\\n* *F* - The file name of the class file from which the log event was issued (if available).\\n* *s* - The stack trace associated with the log event (if present). Although this is a multi-line field, it's formatting is the same as that used by the `printStackTrace()` method.\\n* *p* - The parameters associated with the log event (if present). This is a multi-line field.\\n. *_footer_}* - Optional footer text which is any string which does not contain, but does end with a `}`.\\n\\n[TIP]\\n_The sequence `%%` outputs a single `%`, the sequence `\\\\n` will be replaced by the platform specific line separator, and the sequence `\\\\t` will be replaced by a tab._\\n====\\n\\n[NOTE]\\n====\\nFor example, the following format string...\\n\\n`%d %n %+5l (%.1C.%M:%L) [%T] - %m%!+\\\\n\\\\t!p%!+\\\\n\\\\t!s`\\n\\n...will produce the date, a space, the logger name, a space, the logging level (if the level is less than 5 characters it will br right padded to that length), a space, a left parenthesis, the right-most segment of the name of the calling class, a period, the method name from which the log statement was issued, a colon, the line number at which the log was issued, a right parenthesis, a space, a left bracket, the name of the context thread, a right bracket, a space, a dash, a space, the log message, any parameters available (each one preceded by a new line followed by a tab), and, finally, any stack trace preceded by a new line and tab (if there is a stack trace).\\n====\\n\\n==== Out Of The Box\\n\\nThis project includes a few appenders you may find useful.\\n\\n[[scribe-abstract-appender, AbstractAppender]]\\n===== AbstractAppender\\n\\n[small]#`org.smallmind.scribe.pen.AbstractAppender`#\\n\\nNot an appender in its own right, but a useful base class for complete implementations. This abstract class insures a minimum of correct fields and takes proper care of a few housekeeping chores, like calling an <<scribe-error-handler>> when the `publish()` method fails. In order stsndardize this behavior, _AbstractAppender_ fulfills the `publish()` method, while sub-classes should should implement...\\n\\n[source,java]\\n----\\npublic abstract void handleOutput (Record record)\\n  throws Exception;\\n----\\n\\n[[scribe-error-handler, ErrorHandler]]\\n====== ErrorHandler\\n\\n[small]#`org.smallmind.scribe.pen.ErrorHandler`#\\n\\nAn error handler provides an opportunity for sub-classes of AbstractAppender to find a way to notify client code when the normal log publishing operation fails unexpectedly. When designing an error handler, it's important to keep in mind that notification options may be limited, as the usual venue for logging has just failed. One way to make use of this capability would be to use `org.smallmind.scribe.pen.DefaultErrorHandler`, which takes another appender upon construction and attempts to log the resulting error using this alternate route. Using a <<scribe-console-appender>> as the alternate logger can be a safe bet, although the efficacy of this solution will depend upon how the client code is handling standard out. Creating an error handler is just a matter of implementing...\\n\\n====\\n[source,java]\\n----\\nvoid process (Record record, Exception exception, String errorMessage, Object... args);\\n----\\n\\n...where _record_ is the original log record, _exception_ is the exception thrown from the failed `publish()` method, and the _errorMessage_ and _args_ represent a suggestion for an additional message about the error.\\n====\\n\\n===== AbstractFormattedAppender\\n\\n[small]#`org.smallmind.scribe.pen.AbstractFormattedAppender`#\\n\\nSimply the formatted version of an <<scribe-abstract-appender>>, for completeness and convenience.\\n\\n===== AsynchronousAppender\\n\\n[small]#`org.smallmind.scribe.pen.AsynchronousAppender`#\\n\\nThe AsynchronousAppender is not a complete appender, but rather an appender wrapper which takes `publish()` requests, puts them on a queue, and returns immediately. It holds a background thread which completes the publishing operation asynchronously. To use the AsynchronousAppender you pass its constructor another appender implementation and a buffer size for the queue. If the queue is full at the time the asynchronous appender's `publish()` method is called, an exception will be thrown to that effect.\\n\\n[[scribe-console-appender, ConsoleAppender]]\\n===== ConsoleAppender\\n\\n[small]#`org.smallmind.scribe.pen.ConsoleAppender`#\\n\\nThe ConsoleAppender is a <<scribe-formatted-appender>> that outputs log records to standard out, i.e. _System.out_.\\n\\n===== EmailAppender\\n\\n[small]#`org.smallmind.scribe.pen.EmailAppender`#\\n\\nA <<scribe-formatted-appender>> appender which sends each log record as the body of an email. You should use this judiciously, unless you like a *lot* of email. This appender requires...\\n\\n* *smtpServer* (_String_) - The smtp server host.\\n* *smtpPort* (_int_) - The smtp server port.\\n* *authentication* (_org.smallmind.nutsnbolts.email.Authentication_) - An authentication structure if required by the server.\\n* *secure* (_boolean_) - An optional flag noting that the smtp server is using a secure transport.\\n* *from* (_String_) - The email address of the sender.\\n* *to* (_String_) - The email address of the recipient.\\n* *subject* (_String_) - The subject of the emails.\\n\\n===== FileAppender\\n\\n[small]#`org.smallmind.scribe.pen.FileAppender`#\\n\\nA <<scribe-formatted-appender>> appender which publishes its log records to a file. There are multiple constructors for this class, but in the end the important parameters are...\\n\\n* *logPath* (_java.nio.file.Path_) - The path of the file to which log records are appended, which will be created as necessary.\\n* *rollover* (_<<scribe-rollover>>_) - An object describing the rules for archiving log files whenever they get too large, or too old.\\n* *cleanup* (_<<scribe-cleanup>>_) - An object describing the rules for cleaning up archived log files when they have gotten too old, or too numerous.\\n\\n[[scribe-rollover, Rollover]]\\n====== Rollover\\n\\nA rollover describes the rules for archiving log files which meet the requirements of its rule set. The files will be archived by copying them into the parent of the log path (as siblings of the current log file), with a file name which templates the original file name by adding a timestamp and an ordinal integer, separated by a singe character (which defaults to `-`).\\n\\n[NOTE]\\nFor example, if the original log name is `project.log`, then the archived file might be `project-1996-07-04-0.log`.\\n\\nThe rollover is configured by the following parameters...\\n\\n* *separator* (_char_) - The separator used between the file name, the date and the ordinal index. Defauts to the `-` character.\\n* *timestamp* (_org.smallmind.scribe.pen.Timestamp_) - Controls how dates are formatted. Defaults to \\\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\\\".\\n* *rules* (_org.smallmind.scribe.pen.RolloverRule[]_) - An array of rollover rules. The file will be archived and rolled over if any of the rules is true. This project comes with the following implementations...\\n+\\n====\\n*FileSizeRolloverRule*\\n\\n[small]#`org.smallmind.scribe.pen.FileSizeRolloverRule`#\\n\\nSets the maximum size log files are allowed to reach before being archived and rolled over.\\n====\\n+\\n====\\n*TimestampRolloverRule*\\n\\n[small]#`org.smallmind.scribe.pen.TimestampRolloverRule`#\\n\\nSets the time at which the current log file will be archived and rolled over.\\n====\\n\\n[[scribe-cleanup, Cleanup]]\\n====== Cleanup\\n\\nA cleanup instance describes the rules by which archived logs are deleted. The cleanup is configured with the following parameters...\\n\\n* *separator* (_char_) - The separator used in the rollover for this FileAppender (required so the cleanup can properly parse the file names).\\n* *rules* (_org.smallmind.scribe.pen.CleanupRule[]_) - An array of cleanup rules. Any archived log files that match any of the given rules will be deleted. This project comes with the following implementations...\\n+\\n====\\n*FileCountCleanupRule*\\n\\n[small]#`org.smallmind.scribe.pen.FileCountCleanupRule`#\\n\\nSets the maximum number of archived log files that will kept around. If the number of archived files exceeds the maximum in the rule, then the oldest files will be deleted first, until the total count of files is within bounds.\\n====\\n+\\n====\\n*LastModifiedCleanupRule*\\n\\n[small]#`org.smallmind.scribe.pen.LastModifiedCleanupRule`#\\n\\nProvides the maximum age an archived file is allowed to reach before being deleted.\\n====\\n\\n===== FluentAppender\\n\\n[small]#`org.smallmind.scribe.pen.FluentAppender`#\\n\\nThi appender's output format is the *_forward_* protocol (see https://docs.fluentd.org/input/forward) from FluentD/FluentBit. You'll obviously need a FluentD or FluentBit daemon running somewhere to make this useful. The following parameters are used to configure this appender...\\n\\n* *host* (_String_) - The host on which the FluentD or FluentBit process is running.\\n* *port* (_int_) - The port for the fluent process.\\n* *timestamp* (_org.smallmind.scribe.pen.Timestamp_) - Controls how dates are formatted. Defaults to \\\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\\\".\\n* *newLine* (_String_) - The line separator used to format the multi-line portions of the output. Defaults to `System.getProperty(\\\"line.separator\\\")`.\\n* *retryAttempts* (_int_) - The number of times the appender will attempt to send a batch of log records before giving up.\\n* *batch* (_int) - The number of log records the appender will wait for and batch up into a single send.\\n* *recordElements* (_org.smallmind.scribe.pen.RecordElement[]_) - An array of enum values representing the elements which will be included in the output (same as for the <<scribe-xml-formatter>> above).\\n* *additionalEventData* (_Map<String, String>_) - A map of additional event parameters that will be included in each log record.\\n\\n[[scribe-filters, Filters]]\\n=== Filters\\n\\n[small]#`org.smallmind.scribe.pen.Filter`#\\n\\nBoth <<scribe-logger>> implementations and <<scribe-appenders>> can take filters. To implement a filter you need to fulfill the `willLog()` method...\\n\\n[source,java]\\n----\\nboolean willLog (Record record);\\n----\\n\\nIf any filter in a set returns false for the method above, then the record will not be logged. This project comes with the following filters...\\n\\n==== DotNotatedLoggerNameFilter\\n\\n[small]#`org.smallmind.scribe.pen.DotNotatedLoggerNameFilter`#\\n\\nA filter which allows log records through based on either meeting a particular _<<scribe-level>>_ and/or matching the logger's name with one of the dot notation patterns provided (see `org.smallmind.nutsnbolts.util.DotNotation`). By adding the same instance of this filter to every logger, a client of this project could dynamically control whether log records are output based on the logger name and level associated with each record. This might allow, for example, turning on debug logging across the system, or turning *all* logging on for a particular set of classes or modules.\\n\\n==== LevelFilter\\n\\n[small]#`org.smallmind.scribe.pen.LevelFilter`#\\n\\nA basic level fiter. Log records are passed through that meet or exceed the <<scribe-level>> set on this filter.\\n\\n[[scribe-enhancers, Enhancers]]\\n=== Enhancers\\n\\n[small]#`org.smallmind.scribe.pen.Enhancer`#\\n\\nAn enhancer is essentially a log record decorator. A kind of log record 'get of jail free card', an enhancer can do whatever it wants with a log record by implementing the `enhance()` method...\\n\\n[source,java]\\n----\\nvoid enhance (Record record);\\n----\\n\\n[[scribe-parameters, Parameters]]\\n=== Parameters\\n\\n[small]#`org.smallmind.scribe.pen.adapter.Parameters`#\\n\\nA parameter is a key/value pair, properly held in thread local context, so they are capable of carrying cross-cutting concerns (or at least bits of data about such concerns). Although <<scribe-logger>> implementations may provide alternate integrations to the capabilities of endpoint logging systems, all of those provided by this project use the Parameters class, which is both a factory, and an implementation, of `org.smallmind.scribe.pen.adapter.ParameterAdapter`. To accommodate this behavior, you get the current instance via `Parameters.getInstance()`, upon which you may now call...\\n\\n* `void put (String key, Serializable value)` - Puts a value into the backing thread local map.\\n* `void remove (String key)` - Removes a value from the backing thread local map.\\n* `void clear ()` - Clears the backing thread local map.\\n* `Serializable get (String key)` - Gets a value from the baking thread local map.\\n* `Parameter[] getParameters ()` - Get all parameters currently in the backing thread local map.\\n\\nSee the various implementations of <<scribe-formatter>> for the output of parameters to a log record.\\n\\n[[scribe-logger-manager, LoggerManager]]\\n== LoggerManager\\n\\n[small]#`org.smallmind.scribe.pen.LoggerManager`#\\n\\nThe LoggerManager class is the factory for <<scribe-logger>> instances. It's the static `getLogger()` method which returns an instance of a logger for use, and which takes either a `_String_` or `_Class<?>_` as parameter. The preferred method is to pass it the `_Class_` from which the resulting logger will be called, which makes organizing both loggers and their output relatively natural and tidy. This does mean you end up with, generally, a lot of loggers, which you'll need to configure with the appropriate objects and fields (such as <<scribe-level>>, <<scribe-appenders>>, <<scribe-filters>> and such). Rather than a complex system of hierarchical configurations and inheritances, this project uses <<scribe-templates>>.\\n\\n[[scribe-templates, Templates]]\\n=== Templates\\n\\n[small]#`org.smallmind.scribe.pen.Template`#\\n\\nTemplates can either be statically added to the <<scribe-logger-manager>>, or they will add themselves when their `register()` method is called, usually from the configuring dependency injection framework. Every template vies for the right to configure each logger with the set of objects it contains, with the strongest template winning. A template has methods for conveniently setting, and is a subsequent container for, the following information...\\n\\n* *Appenders* (_<<scribe-appenders>>_) - A list of appenders which will be set on any matching logger.\\n* *AutoFillLoggerContext* (_boolean_) - Whether the matching logger will auto-fill its <<scribe-logger-context>>. Defaults to false.\\n* *Enhancers* (_<<scribe-enhancers>>_) - A list of enhancers which will be set on any matching logger.\\n* *Filters* (_<<scribe-filters>>_) - A list of filters which will be set on any matching logger.\\n* *Level* (_<<scribe-level>>_) - The default level for any matching logger. Defaults to _Level.INFO_.\\n\\nWe recommend generating a default template, which acts as a fallback default configuration, and then a set of templates which will bind themselves to the appropriate hierarchically named loggers as they are requested. Such a setup is not hard given the available template implementations.\\n\\n[[scribe-class-name-template, ClassNameTemplate]]\\n==== ClassNameTemplate\\n\\n[small]#`org.smallmind.scribe.pen.ClassNameTemplate`#\\n\\nThis template takes a dot-notated pattern upon construction (see `org.smallmind.nutsnbolts.util.DotNotation`), and binds to loggers based on the strength of the match with their names (which should, obviously, be dot notated). The binding strength is proportional to the number of matching segments in the pattern, with wild card segments valued as slightly weaker.\\n\\n==== DefaultTemplate\\n\\n[small]#`org.smallmind.scribe.pen.DefaultTemplate`#\\n\\nThis template will match any logger at the weakest possible binding value.\\n\\n==== PeronalizedTemplate\\n\\n[small]#`org.smallmind.scribe.pen.PersonalizedTemplate`#\\n\\nThis template takes a name upon construction and is all or nothing, matching any logger with exactly the same name, at the strongest possible binding value.\\n\\n==== RegExTemplate\\n\\n[small]#`org.smallmind.scribe.pen.RegexTemplate`#\\n\\nAlthough the <<scribe-class-name-template>> is more flexible, and in general a better choice, this template can be used when logger names do not follow dot-notated conventions. This template take a regular expression upon construction, and binds to loggers whose names match the regular expression. The binding value is all or nothing, and will bind at the maximum strength if there's a match.\\n\\n== Configuration\\n\\n== Adaptation\\n\\n[[scribe-indigenous, Indigenous]]\\n=== Indigenous\\n\\n=== JDK Logging\\n\\n=== Log4J\\n\\n=== Blueprint\\n\\n=== LoggerAdapter\\n\\n[[scribe-record, Record]]\\n=== Record\\n\\n=== Parameter Adapter\\n\\n== Apache Commons Logging Integration\\n\\n== SLF4J Integration\\n\\n[[spark, Spark]]\\n= Spark\\n\\n[partintro]\\nThe Spark project consists of Maven packaging formats for the construction of self-contained executable build artifacts.\\n\\n[[spark-singularity, Singularity]]\\n== Singularity\\n\\nSingularity is a Maven packaging format for executable jars *with* all their dependencies. Notably, a Singularity package does not unpack and repack its dependencies. The original jar files are maintained, and their namespaces are preserved. In order to speed class resolution and loading, indexes are built on compilation through a bit of Maven plugin magic. There should be nothing special you need to do with your code to create a Singulairty from it, other than defining the packaging and nominating a `public static void main (String... args)` entry point.\\n\\n=== Install\\n\\nIn order to have Maven create a self-contained jar you should declare the packaging as `singularity` and include the `spark-singularity-maven-plugin` as outlined below..\\n\\n.Singularity Plugin\\n[source,xml]\\n----\\n<project>\\n  ...\\n  <packaging>singularity</packaging>\\n  ...\\n  <build>\\n    <plugins>\\n      <plugin>\\n        <groupId>org.smallmind</groupId>\\n        <artifactId>spark-singularity-maven-plugin</artifactId>\\n        <version>LATEST</version>\\n        <extensions>true</extensions>\\n        <configuration>\\n          <mainClass><!-- entry point class containing a main() method --></mainClass>\\n        </configuration>\\n      </plugin>\\n    </plugins>\\n  </build>\\n</project>\\n----\\n\\n=== Configuration\\n\\nThe following configuration attributes are supported by the plugin...\\n\\n* *mainClass* (required) - An entry point class containing a standard `public static void main (String... args)` method.\\n+\\n.Example\\n[source,xml]\\n----\\n<mainClass>my.Main</mainClass>\\n----\\n\\n* *skip* (optional, defaults to _false_) - If this attribute exists and is set `true`, then the plugin will skip its operations and no artifact will be produced.\\n+\\n.Example\\n[source,xml]\\n----\\n<skip>true</skip>\\n----\";","<template>\n  <span v-html=\"documentation\"></span>\n</template>\n\n<script>\n  import raw from \"raw-loader!@/assets/adoc/3.6.0/README.adoc\";\n  const asciidoctor = require(\"asciidoctor\")();\n\n  export default {\n    data() {\n      return {\n        documentation: asciidoctor.convert(raw, {\n          doctype: \"book\",\n          standalone: true,\n          safe: \"safe\"\n        })\n      };\n    }\n  };\n</script>\n","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Docs.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Docs.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Docs.vue?vue&type=template&id=7bb12adc&\"\nimport script from \"./Docs.vue?vue&type=script&lang=js&\"\nexport * from \"./Docs.vue?vue&type=script&lang=js&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","<template>\n  <Docs/>\n</template>\n\n<script>\n  import Docs from \"@/components/Docs.vue\";\n\n  export default {\n    name: \"Home\",\n    components: {\n      Docs\n    }\n  };\n</script>\n","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Home.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Home.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Home.vue?vue&type=template&id=da9653f4&\"\nimport script from \"./Home.vue?vue&type=script&lang=js&\"\nexport * from \"./Home.vue?vue&type=script&lang=js&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports"],"sourceRoot":""}